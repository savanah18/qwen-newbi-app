# Model Server Configuration

# Loading Strategy: native or vllm
LOADING_STRATEGY=native

# Quantization Type: none, int4, int8, awq, gptq, fp8, squeezellm
# - none: Full precision (FP16/BF16) - works with any model
# - int4: 4-bit quantization (BitsAndBytes, native only) - works with any model
# - int8: 8-bit quantization (BitsAndBytes, native only) - works with any model
# - awq: AWQ quantization (vLLM only, REQUIRES pre-quantized AWQ model)
# - gptq: GPTQ quantization (vLLM only, REQUIRES pre-quantized GPTQ model)
# - fp8: FP8 quantization (vLLM only, H100+ GPU, works with any model)
# - squeezellm: SqueezeLLM (vLLM only, REQUIRES pre-quantized SqueezeLLM model)
#
# IMPORTANT: AWQ, GPTQ, and SqueezeLLM require models that have been pre-quantized.
# You cannot use these with a standard model checkpoint.
# Set to 'none' if you don't have a quantized model.
QUANTIZATION_TYPE=int4

# Attention Implementation: eager, sdpa, flash_attention_2 (native only)
ATTENTION_IMPL=sdpa

# Model Path
MODEL_PATH=/root/workspace/lnd/aiops/vlm/Qwen/Qwen3-VL-8B-Instruct

# Conversation Settings
MAX_HISTORY=5
MAX_NEW_TOKENS=512

# vLLM Specific Settings (only used when LOADING_STRATEGY=vllm)
VLLM_GPU_MEMORY_UTILIZATION=0.9
VLLM_TENSOR_PARALLEL_SIZE=1
