# Model Server Configuration

# Loading Strategy: native or vllm
LOADING_STRATEGY=native

# Quantization Type: none, int4, int8, awq, gptq
# - none: Full precision (FP16/BF16)
# - int4: 4-bit quantization (BitsAndBytes, native only)
# - int8: 8-bit quantization (BitsAndBytes, native only)
# - awq: AWQ quantization (vLLM only, requires pre-quantized model)
# - gptq: GPTQ quantization (vLLM only, requires pre-quantized model)
QUANTIZATION_TYPE=int4

# Attention Implementation: eager, sdpa, flash_attention_2 (native only)
ATTENTION_IMPL=sdpa

# Model Path
MODEL_PATH=/root/workspace/lnd/aiops/vlm/Qwen/Qwen3-VL-8B-Instruct

# Conversation Settings
MAX_HISTORY=5
MAX_NEW_TOKENS=512

# vLLM Specific Settings (only used when LOADING_STRATEGY=vllm)
VLLM_GPU_MEMORY_UTILIZATION=0.9
VLLM_TENSOR_PARALLEL_SIZE=1
